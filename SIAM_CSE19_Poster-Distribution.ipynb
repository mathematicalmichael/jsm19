{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> SIAM CSE Poster: Param Distribution Part"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Requires:**\n",
    "* `pymc3` for the hierarchical bayes modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymc3 as pm\n",
    "import numpy as np\n",
    "import scipy.stats as sps\n",
    "from scipy.integrate import quad\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate data from a `double_tent` pdf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class double_tent:\n",
    "    ''' double_tent defines a pdf class object on 0,1 with two triangular peaks\n",
    "        loc: defines the location of two peaks\n",
    "        weight: defines the height of peaks. Two hieghts must sum to 1.\n",
    "        Requires: scipy.stats.trapz\n",
    "    '''\n",
    "    \n",
    "    def __init__(self,loc,weight):\n",
    "        # check that locations and weights are appropriate\n",
    "        if loc[0]<0 or loc[0]>1:\n",
    "            raise ValueError('Peaks must be in [0,1]')\n",
    "        elif loc[1]<0 or loc[1]>1:\n",
    "            raise ValueError('Peaks must be in [0,1]')\n",
    "        \n",
    "        if weight[0]+weight[1]!=1:\n",
    "            raise ValueError('Weights must sum to 1')\n",
    "        \n",
    "        self.loc = loc # defines two peaks of pdf\n",
    "        self.weight = weight # defines two heights of pdf\n",
    "        \n",
    "        # defines pdf function\n",
    "        self.pdf_fun = lambda x: weight[0]*sps.trapz.pdf(x,loc[0],loc[0],loc=0,scale=0.385)+ \\\n",
    "                        weight[1]*sps.trapz.pdf(x,loc[1],loc[1],loc=0.8,scale=0.2)\n",
    "    \n",
    "    \n",
    "    def pdf(self,x):\n",
    "        return self.pdf_fun(x)\n",
    "    \n",
    "    def rvs(self,N):\n",
    "        '''Generates N samples from the pdf'''\n",
    "        # accept reject for sample from f\n",
    "        sample = np.ones(N)*np.NaN\n",
    "        i=0 # index for while loop\n",
    "\n",
    "        while i<N:\n",
    "            test = np.random.uniform(0,1) # test value\n",
    "\n",
    "            # acceptance criteria\n",
    "            if np.random.uniform(0,1)<self.pdf(test)/4:\n",
    "                sample[i]=test\n",
    "                i+=1\n",
    "\n",
    "        return sample\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### FIXED PARAMETERS - DEFINE YOUR EXPERIMENT #####\n",
    "start_time = 1\n",
    "end_time = 3\n",
    "sigma2 = 1E-3\n",
    "sigma = np.sqrt(sigma2) # fixed noise level in the data\n",
    "data_n = 150\n",
    "sample_size = 800\n",
    "####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate the Data\n",
    "\n",
    "We generate data from a double tent function $\\lambda$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # define target distribution and noise distribution\n",
    "# target_alpha = 2\n",
    "# target_beta = 5\n",
    "\n",
    "# define the target distribution\n",
    "lam_dist = double_tent(loc=[0.6,0.2],weight=[0.75,0.25])\n",
    "noise_dist = sps.norm(0,sigma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(-0.1,1.1,100)\n",
    "plt.plot(x,lam_dist.pdf(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The $Q$ map is:\n",
    "\n",
    "\\begin{align}\n",
    "Q(\\lambda,\\delta)=0.5\\cdot \\exp(-\\lambda t)+\\delta\n",
    "\\end{align}\n",
    "\n",
    "where we assume $t=0.5$ is constant and $\\delta$ is a random variable representing a noise parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defines the map from parameters to data\n",
    "def data_map(param,t=np.array([2])):\n",
    "    q_map = 0.5*np.exp(-t*param)\n",
    "    noise = noise_dist.rvs(size=q_map.shape)\n",
    "    q_out = q_map+noise\n",
    "    return q_out, (q_map,noise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gets a sample of lambda for the observed data\n",
    "lam_sample = lam_dist.rvs(data_n)\n",
    "data_sample, (this_q,this_noise) = data_map(lam_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10,8))\n",
    "ax = fig.add_subplot(2,2,1)\n",
    "ax.scatter(lam_sample,this_q)\n",
    "\n",
    "ax = fig.add_subplot(2,2,2)\n",
    "ax.set_title('$\\lambda$ Dist: Doubletent')\n",
    "ax.hist(lam_sample,edgecolor='k')\n",
    "\n",
    "ax = fig.add_subplot(2,2,3)\n",
    "ax.scatter(lam_sample,data_sample)\n",
    "\n",
    "ax = fig.add_subplot(2,2,4)\n",
    "ax.set_title('Data Distribution')\n",
    "ax.hist(data_sample,edgecolor='k')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Infer Using Hierarchical Parameteric Bayes\n",
    "\n",
    "Here we use the following parametric Bayesian model:\n",
    "\n",
    "\\begin{align}\n",
    "\\alpha,\\beta\\ &\\sim  \\chi^2(df=1) \\\\\n",
    "\\lambda\\ \\mid\\ \\alpha,\\beta &\\sim \\text{beta_distr}(\\alpha,\\beta) \\\\\n",
    "d\\ \\mid \\ \\lambda, (\\alpha,\\beta) &\\sim N(Q(\\lambda),\\sigma^2)\n",
    "\\end{align}\n",
    "\n",
    "which leads to:\n",
    "\\begin{align}\n",
    "\\pi^{post}(\\lambda\\mid \\{d_1,\\ldots,d_n\\})&\\propto \\int_{\\Omega}\\pi^{prior}(\\lambda\\mid \\alpha,\\beta)\\cdot\\pi^{prior}(\\alpha,\\beta)\\cdot \\pi^{likelihood}(\\{d_1,\\ldots,d_n\\}\\mid \\lambda,(\\alpha,\\beta))\\ d\\Omega\n",
    "\\end{align}\n",
    "\n",
    "where $(\\alpha,\\beta)\\in\\Omega$ is the $[0,\\infty]\\times[0,\\infty]$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pm.Model() as model:\n",
    "    ab = pm.ChiSquared('hyper',nu=1,shape=(2,))#pm.Uniform('hyper',lower=0,upper=10,shape=(2,))\n",
    "    lam = pm.Beta('lambda',alpha=ab[0],beta=ab[1], shape=data_n)\n",
    "    Q_map = pm.Deterministic('Q',0.5*pm.math.exp(-2*lam))\n",
    "    dat = pm.Normal('data',mu=Q_map,sigma=sigma,observed=data_sample)\n",
    "    trace = pm.sample(500,tune=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saves the posterior bayes sample as single array\n",
    "# and computes the posterior-predictive sample\n",
    "bayes_sample = trace['lambda'].reshape(-1,)\n",
    "ppc = pm.sample_posterior_predictive(trace, samples=500, model=model)\n",
    "pp_sample = ppc['data'].reshape(-1,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(trace['hyper'][:,0],trace['hyper'][:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plots the original target sample versus bayes sample\n",
    "plt.hist(lam_sample,density=True,edgecolor='k',label='true sample')\n",
    "plt.hist(bayes_sample,density=True,edgecolor='k',alpha=0.4,label='bayes sample')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plots posterior predictive sample vs. data sample\n",
    "plt.hist(data_sample,density=True,edgecolor='k',label=\"data sample\")\n",
    "plt.hist(pp_sample,density=True,alpha=0.5,edgecolor='k',label='pp sample')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Infer Using Data Consistent Version\n",
    "\n",
    "Data Consistent model looks like:\n",
    "\n",
    "\\begin{align}\n",
    "\\pi^{init}(\\lambda) &\\sim U[0,1] \\Leftrightarrow \\text{beta_distr}(\\alpha=1,\\beta=1) \\\\\n",
    "\\pi^{obs}(Q(\\lambda)) &\\sim \\pi(d)\\text{, estimated via KDE} \\\\\n",
    "\\pi^{pf}(Q(\\lambda))&\\sim \\text{can be estimated using samples of lambda, KDE}\n",
    "\\end{align}\n",
    "\n",
    "The updated distribution will be:\n",
    "\\begin{align}\n",
    "\\pi^{update}(\\lambda)&=\\pi^{init}(\\lambda)\\cdot\\dfrac{\\pi^{obs}(Q(\\lambda))}{\\pi^{pf}(Q(\\lambda))}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define prior distribution\n",
    "lam_init = sps.beta(a=1,b=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get prior sample and push-forward sample\n",
    "lam_init_sample = lam_init.rvs(18000)\n",
    "pf_init_sample, z = data_map(lam_init_sample)\n",
    "\n",
    "# estimate the data distribution and push-forward using KDE\n",
    "data_dist = sps.gaussian_kde(data_sample)\n",
    "pf_dist = sps.gaussian_kde(pf_init_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# accept-reject algorithm\n",
    "# calculate maximum of the ratio\n",
    "M = np.max(data_dist(pf_init_sample)/pf_dist(pf_init_sample))\n",
    "\n",
    "# generate random numbers from uniform for accept-reject for each sample value\n",
    "test_value = np.random.uniform(0,1,np.shape(pf_init_sample))\n",
    "\n",
    "# calculate the ratio for accept reject: data_kde/push_kde/M and compare to test sample\n",
    "# is the kde ratio > test value?\n",
    "accept_or_reject_samples = np.greater(data_dist(pf_init_sample)/pf_dist(pf_init_sample)/M,\n",
    "                            test_value)\n",
    "\n",
    "# accepted values of posterior sample\n",
    "updated_sample = lam_init_sample[accept_or_reject_samples]\n",
    "\n",
    "print('Acceptance Rate about 1/5: update samples = ', np.shape(updated_sample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# computes the push-forward of the updated sample\n",
    "pf_update,_ = data_map(updated_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plots the original target sample versus update sample\n",
    "plt.figure()\n",
    "plt.hist(lam_sample,density=True,edgecolor='k',label='true sample')\n",
    "plt.hist(updated_sample,density=True,edgecolor='k',label='update sample',alpha=0.5)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plots data sample versus push-forward of update sample\n",
    "plt.hist(data_sample,density=True,edgecolor='k',label=\"data sample\")\n",
    "plt.hist(pf_update,density=True,alpha=0.5,edgecolor='k',label='pf-update sample')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wrong Bayesian Model\n",
    "\n",
    "A Bayesian model that is not correct but \"looks\" more like Data Consistent in terms of form:\n",
    "\n",
    "\\begin{align}\n",
    "\\lambda &\\sim \\text{beta_distr}(\\alpha=1,\\beta=1) \\ [\\text{uniform prior}]\\\\\n",
    "d\\ \\mid \\ \\lambda &\\sim N(Q(\\lambda),\\sigma^2)\n",
    "\\end{align}\n",
    "\n",
    "Then the posterior will be:\n",
    "\\begin{align}\n",
    "\\pi^{posterior}(\\lambda\\mid \\{d_1,\\ldots,d_n\\})\\propto \\pi^{prior}(\\lambda)\\cdot \\pi^{likelihood}(\\{d_1,\\ldots,d_n\\}\\mid \\lambda)\n",
    "\\end{align}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pm.Model() as bad_model:\n",
    "    lam2 = pm.Beta('lambda',alpha=1,beta=1)\n",
    "    Q_map2 = pm.Deterministic('Q',0.5*pm.math.exp(-2*lam2))\n",
    "    dat2 = pm.Normal('data',mu=Q_map2,sigma=sigma,observed=data_sample)\n",
    "    bad_trace = pm.sample(500,tune=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saves the posterior bayes sample as single array\n",
    "# and computes the posterior-predictive sample\n",
    "bad_bayes_sample = bad_trace['lambda'].reshape(-1,)\n",
    "bad_ppc = pm.sample_posterior_predictive(bad_trace, samples=500, model=bad_model)\n",
    "bad_pp_sample = bad_ppc['data'].reshape(-1,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(bad_bayes_sample,edgecolor='k',density=True)\n",
    "print('Mean: ', np.mean(bad_bayes_sample))\n",
    "print('Variance: ', np.mean(bad_bayes_sample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# plots the original target sample versus bayes sample\n",
    "plt.hist(lam_sample,density=True,edgecolor='k',label='true sample')\n",
    "plt.hist(bad_bayes_sample,density=True,edgecolor='k',alpha=0.4,label='bayes sample')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plots posterior predictive sample vs. data sample\n",
    "plt.hist(data_sample,density=True,edgecolor='k',label=\"data sample\")\n",
    "plt.hist(bad_pp_sample,density=True,alpha=0.5,edgecolor='k',label='pp sample')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparison of Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(seaborn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams.update({'font.size': 42})\n",
    "plt.rcParams['figure.figsize'] = 20, 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get prior sample for plotting hyper prior\n",
    "hyper_prior_sample = sps.chi2.rvs(df=1,size=(2000,2000))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the hyperprior and posterior sample\n",
    "seaborn.set_style(\"white\")\n",
    "seaborn.set_context(\"poster\")\n",
    "\n",
    "# plot hyper prior\n",
    "hyper_figure = seaborn.jointplot(hyper_prior_sample[0],hyper_prior_sample[1],\n",
    "                      kind='kde', xlim=(0,3), ylim=(0,3),color='C0',label='Hyper prior sample')\n",
    "\n",
    "# plot the posterior sample\n",
    "hyper_figure.ax_joint.scatter(trace['hyper'][:,0],trace['hyper'][:,1],\n",
    "                              color='red',label='Posterior sample')\n",
    "hyper_figure.ax_joint.legend()\n",
    "hyper_figure.fig.set_figheight(12)\n",
    "hyper_figure.fig.set_figwidth(12)\n",
    "hyper_figure.ax_joint.set_xlabel('$\\\\alpha$')\n",
    "hyper_figure.ax_joint.set_ylabel('$\\\\beta$')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# posterior kdes for plotting\n",
    "posterior_kde=sps.gaussian_kde(bayes_sample)\n",
    "bad_posterior_kde=sps.gaussian_kde(bad_bayes_sample)\n",
    "updated_dist = sps.gaussian_kde(updated_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the objects of other spaces\n",
    "fig_objects,axes = plt.subplots(1,3,sharey=True)\n",
    "fig_objects.set_figheight(8)\n",
    "\n",
    "### set other params\n",
    "# color scheme\n",
    "c_prior = 'blue'\n",
    "c_post = 'red'\n",
    "\n",
    "\n",
    "#### plot the parameter space (first row)\n",
    "x_lam = np.linspace(-0.1,1.1,250)\n",
    "\n",
    "## HIERARCHICAL BAYES\n",
    "# prior for hierarchical bayes\n",
    "ax = axes[0]\n",
    "# ab_plot = [(0.5,0.5),(1,1),(5,1),(1,3),(2,2)]\n",
    "# for ab in ab_plot:\n",
    "#     ax.plot(x_lam,sps.beta.pdf(x_lam,a=ab[0],b=ab[1]),\n",
    "#                    label='Prior $\\\\alpha,\\\\beta = $'+str(ab))\n",
    "\n",
    "ax.plot(x_lam,sps.beta.pdf(x_lam,a=1,b=1),color=c_prior,\n",
    "                   label='Prior $\\mid \\\\alpha,\\\\beta = $'+str((1,1)))\n",
    "\n",
    "# posterior for hierarchical bayes\n",
    "ax.hist(bayes_sample,density=True,color=c_post,alpha=0.7)\n",
    "ax.plot(x_lam,posterior_kde(x_lam),label='Posterior',color=c_post)\n",
    "ax.legend()\n",
    "\n",
    "\n",
    "## DATA CONSISTENT\n",
    "ax = axes[1]\n",
    "# Initial for Data Consistent\n",
    "ax.plot(x_lam,lam_init.pdf(x_lam),label='Initial',color=c_prior)\n",
    "\n",
    "# Update for Data Consistent\n",
    "ax.hist(updated_sample,density=True,color=c_post,alpha=0.7)\n",
    "ax.plot(x_lam,updated_dist(x_lam),label='Update',color=c_post)\n",
    "ax.legend()\n",
    "\n",
    "\n",
    "## Bad Posterior\n",
    "ax = axes[2]\n",
    "\n",
    "# Prior for Bad Bayes\n",
    "ax.plot(x_lam,lam_init.pdf(x_lam),label='Prior',color=c_prior)\n",
    "\n",
    "# Posterior for Bad Bayes\n",
    "ax.hist(bad_bayes_sample,density=True,color=c_post, edgecolor=c_post,alpha=0.7)\n",
    "ax.plot(x_lam,bad_posterior_kde(x_lam),label='Posterior',color=c_post)\n",
    "ax.set_ylim(0,3.5)\n",
    "ax.legend()\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# posterior predictive KDEs\n",
    "pp_kde=sps.gaussian_kde(pp_sample)\n",
    "bad_pp_kde=sps.gaussian_kde(bad_pp_sample)\n",
    "pf_update_kde=sps.gaussian_kde(pf_update)\n",
    "likelihood_pdf = sps.norm(0.5*np.exp(-2*0.5*0.5),sigma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the objects of other spaces\n",
    "fig_data,axes = plt.subplots(1,3,sharey=True)\n",
    "\n",
    "\n",
    "fig_data.set_figheight(8)\n",
    "\n",
    "### set other params\n",
    "# color scheme\n",
    "c_data = 'blue'\n",
    "c_data_use = 'pink'\n",
    "c_pf = 'green'\n",
    "c_post = 'red'\n",
    "\n",
    "\n",
    "#### plot the parameter space (first row)\n",
    "x_q = np.linspace(-0.1,0.55,250)\n",
    "\n",
    "for ax in axes:\n",
    "    ax.hist(data_sample,density=True,label='Data',color=c_data)\n",
    "\n",
    "## HIERARCHICAL BAYES\n",
    "# posterior predictive for hierarchical bayes\n",
    "ax = axes[0]\n",
    "ax.plot(x_q,1/4*likelihood_pdf.pdf(x_q),color=c_data_use,\n",
    "        label='Likelihood$\\mid \\\\lambda = 0.5$')\n",
    "ax.plot(x_q,pp_kde(x_q),label='Posterior Predictive',color=c_post)\n",
    "ax.legend()\n",
    "\n",
    "\n",
    "# ## DATA CONSISTENT\n",
    "ax = axes[1]\n",
    "# Push-forward and Obeserved\n",
    "ax.plot(x_q,data_dist(x_q),color=c_data_use,label='Observed')\n",
    "ax.plot(x_q,pf_dist(x_q),label='Pushforward',color=c_pf)\n",
    "ax.legend()\n",
    "\n",
    "\n",
    "# ## Bad Posterior\n",
    "ax = axes[2]\n",
    "\n",
    "# Posterior Predictive\n",
    "ax.plot(x_q,1/4*likelihood_pdf.pdf(x_q),color=c_data_use,\n",
    "        label='Likelihood$\\mid \\\\lambda = 0.5$')\n",
    "ax.plot(x_q,bad_pp_kde(x_q),label='Posterior Predictive',color=c_post)\n",
    "ax.set_ylim(0,6.5)\n",
    "ax.legend()\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_fig, axes = plt.subplots(1,2)\n",
    "final_fig.set_figheight(10)\n",
    "\n",
    "bayes_color = 'green'\n",
    "dci_color = 'orange'\n",
    "\n",
    "# compare in lambda\n",
    "ax = axes[0]\n",
    "ax.plot(x_lam,posterior_kde(x_lam),label='Posterior',color=bayes_color)\n",
    "ax.plot(x_lam,updated_dist(x_lam),label='Update',color=dci_color)\n",
    "ax.legend()\n",
    "\n",
    "# compare in data\n",
    "ax = axes[1]\n",
    "ax.hist(data_sample,density=True,label='Data',color=c_data)\n",
    "ax.plot(x_q,data_dist(x_q),color=dci_color,label='P.f. Update')\n",
    "ax.plot(x_q,pp_kde(x_q),label='Posterior Predictive',color=bayes_color)\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
